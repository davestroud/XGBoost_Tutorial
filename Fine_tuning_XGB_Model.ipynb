{"cells":[{"cell_type":"markdown","source":["# Extreme Gradient Boosting with XGBoost"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dea9cb9e-2979-49cf-863f-fec1fba2d044"}}},{"cell_type":"markdown","source":["## Fine-tuning your XGBoost model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3b159f04-05b1-45a2-b1a3-88c1fc490dd6"}}},{"cell_type":"code","source":["import xgboost as xgb\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01cbb52a-88d0-4e36-bb58-7f6388dc108f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["california_housing = fetch_california_housing(as_frame=True)\n\n# Convert to Pandas\nhousing = pd.DataFrame(data=california_housing.data, columns=california_housing.feature_names)\nhousing[\"target\"] = california_housing.target\nhousing.head()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"216fc5e3-7d2a-4707-9f37-05cf32b55989"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[2]: </div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[2]: </div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.3252</td>\n      <td>41.0</td>\n      <td>6.984127</td>\n      <td>1.023810</td>\n      <td>322.0</td>\n      <td>2.555556</td>\n      <td>37.88</td>\n      <td>-122.23</td>\n      <td>4.526</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.3014</td>\n      <td>21.0</td>\n      <td>6.238137</td>\n      <td>0.971880</td>\n      <td>2401.0</td>\n      <td>2.109842</td>\n      <td>37.86</td>\n      <td>-122.22</td>\n      <td>3.585</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.2574</td>\n      <td>52.0</td>\n      <td>8.288136</td>\n      <td>1.073446</td>\n      <td>496.0</td>\n      <td>2.802260</td>\n      <td>37.85</td>\n      <td>-122.24</td>\n      <td>3.521</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.6431</td>\n      <td>52.0</td>\n      <td>5.817352</td>\n      <td>1.073059</td>\n      <td>558.0</td>\n      <td>2.547945</td>\n      <td>37.85</td>\n      <td>-122.25</td>\n      <td>3.413</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.8462</td>\n      <td>52.0</td>\n      <td>6.281853</td>\n      <td>1.081081</td>\n      <td>565.0</td>\n      <td>2.181467</td>\n      <td>37.85</td>\n      <td>-122.25</td>\n      <td>3.422</td>\n    </tr>\n  </tbody>\n</table>\n</div>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MedInc</th>\n      <th>HouseAge</th>\n      <th>AveRooms</th>\n      <th>AveBedrms</th>\n      <th>Population</th>\n      <th>AveOccup</th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>8.3252</td>\n      <td>41.0</td>\n      <td>6.984127</td>\n      <td>1.023810</td>\n      <td>322.0</td>\n      <td>2.555556</td>\n      <td>37.88</td>\n      <td>-122.23</td>\n      <td>4.526</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8.3014</td>\n      <td>21.0</td>\n      <td>6.238137</td>\n      <td>0.971880</td>\n      <td>2401.0</td>\n      <td>2.109842</td>\n      <td>37.86</td>\n      <td>-122.22</td>\n      <td>3.585</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.2574</td>\n      <td>52.0</td>\n      <td>8.288136</td>\n      <td>1.073446</td>\n      <td>496.0</td>\n      <td>2.802260</td>\n      <td>37.85</td>\n      <td>-122.24</td>\n      <td>3.521</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.6431</td>\n      <td>52.0</td>\n      <td>5.817352</td>\n      <td>1.073059</td>\n      <td>558.0</td>\n      <td>2.547945</td>\n      <td>37.85</td>\n      <td>-122.25</td>\n      <td>3.413</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3.8462</td>\n      <td>52.0</td>\n      <td>6.281853</td>\n      <td>1.081081</td>\n      <td>565.0</td>\n      <td>2.181467</td>\n      <td>37.85</td>\n      <td>-122.25</td>\n      <td>3.422</td>\n    </tr>\n  </tbody>\n</table>\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Tuning the number of boosting rounds\n- Create a DMatrix called housing_dmatrix from X and y.\n- Create a parameter dictionary called params, passing in the appropriate \"objective\" (\"reg:squarederror\") and \"max_depth\" (set it to 3).\n- Iterate over num_rounds inside a for loop and perform 3-fold cross-validation. In each iteration of the loop, pass in the current number of boosting rounds (curr_num_rounds) to xgb.cv() as the argument to num_boost_round.\n- Append the final boosting round RMSE for each cross-validated XGBoost model to the final_rmse_per_round list.\n- num_rounds and final_rmse_per_round have been zipped and converted into a DataFrame so you can easily see how the model performs with each boosting round."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a6bf9395-3557-4863-b1d9-8fc59f7a61c7"}}},{"cell_type":"code","source":["X, y = housing[housing.columns.to_list()[:-1]],housing[housing.columns.to_list()[-1]]\n\n# Create the DMatrix: housing_dmatrix\nhousing_dmatrix = xgb.DMatrix(data=X, label=y)\n\n# Create the parameter dictionary for each tree: params \nparams = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n\n# Create list of number of boosting rounds\nnum_rounds = [5, 10, 15]\n\n# Empty list to store final round rmse per XGBoost model\nfinal_rmse_per_round = []\n\n# Iterate over num_rounds and build one model per num_boost_round parameter\nfor curr_num_rounds in num_rounds:\n\n    # Perform cross-validation: cv_results\n    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n    \n    # Append final round RMSE\n    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nnum_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\nprint(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf496c03-fd10-4ade-ac50-ce627416f37f"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">   num_boosting_rounds      rmse\n0                    5  0.781960\n1                   10  0.622003\n2                   15  0.581889\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">   num_boosting_rounds      rmse\n0                    5  0.781960\n1                   10  0.622003\n2                   15  0.581889\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Automated boosting round selection using early_stopping\n\nEarly stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the early_stopping_rounds parameter in xgb.cv() with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when num_boost_rounds is reached, then early stopping does not occur."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22a842fc-bf76-4e10-b3d0-9654426f6e95"}}},{"cell_type":"code","source":["# Create the parameter dictionary for each tree: params\nparams = {\"objective\":\"reg:squarederror\", \"max_depth\":4}\n\n# Perform cross-validation with early stopping: cv_results\ncv_results = xgb.cv(dtrain=housing_dmatrix, params = params, nfold=3, num_boost_round = 50,\n                   early_stopping_rounds=10, metrics=\"rmse\", as_pandas=True, seed=123)\n\n# Print cv_results\nprint(cv_results)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a778a6a5-b691-4f98-85af-2430bb791b10"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n0          1.462828        0.005529        1.465339       0.011217\n1          1.143124        0.004570        1.150056       0.009817\n2          0.939146        0.003773        0.948381       0.007459\n3          0.808610        0.004855        0.821553       0.007730\n4          0.725585        0.006141        0.739874       0.005441\n5          0.671414        0.004638        0.688221       0.005762\n6          0.633774        0.003228        0.650904       0.008133\n7          0.606048        0.008341        0.624120       0.008889\n8          0.586223        0.008837        0.605858       0.006527\n9          0.568964        0.005324        0.589065       0.011413\n10         0.555295        0.002455        0.576495       0.008162\n11         0.546593        0.002349        0.570481       0.009861\n12         0.540629        0.001898        0.566814       0.011006\n13         0.528651        0.003696        0.555914       0.008361\n14         0.523607        0.002623        0.552355       0.010605\n15         0.519032        0.001862        0.548983       0.010796\n16         0.514179        0.000913        0.545007       0.011478\n17         0.508824        0.001588        0.541242       0.012881\n18         0.504964        0.002693        0.538327       0.011753\n19         0.499853        0.003439        0.534938       0.010439\n20         0.496571        0.003976        0.533044       0.009761\n21         0.493573        0.003483        0.531449       0.010813\n22         0.491831        0.003357        0.530603       0.010938\n23         0.488524        0.002799        0.528553       0.011084\n24         0.485174        0.002016        0.526113       0.011790\n25         0.481729        0.002600        0.523945       0.011225\n26         0.480254        0.002262        0.523230       0.011452\n27         0.477783        0.001902        0.521776       0.011676\n28         0.474902        0.003005        0.519439       0.010713\n29         0.472486        0.002938        0.518332       0.009949\n30         0.470619        0.003347        0.517276       0.009942\n31         0.468716        0.003182        0.515970       0.009670\n32         0.466677        0.003220        0.515286       0.009412\n33         0.465271        0.003460        0.514172       0.009016\n34         0.463104        0.003572        0.512874       0.008700\n35         0.461313        0.004143        0.512332       0.008410\n36         0.459000        0.003781        0.510704       0.008571\n37         0.457332        0.003890        0.509791       0.008110\n38         0.455053        0.003804        0.508612       0.008641\n39         0.453656        0.003941        0.507963       0.008456\n40         0.451406        0.003268        0.507021       0.008810\n41         0.449972        0.003077        0.506284       0.009194\n42         0.448604        0.002691        0.505271       0.009450\n43         0.447176        0.002342        0.504704       0.009811\n44         0.446304        0.002042        0.504476       0.009743\n45         0.444298        0.002090        0.502807       0.009778\n46         0.442912        0.002268        0.502411       0.009483\n47         0.441760        0.002180        0.502340       0.010093\n48         0.440237        0.002350        0.501567       0.009791\n49         0.437942        0.001547        0.500168       0.010706\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n0          1.462828        0.005529        1.465339       0.011217\n1          1.143124        0.004570        1.150056       0.009817\n2          0.939146        0.003773        0.948381       0.007459\n3          0.808610        0.004855        0.821553       0.007730\n4          0.725585        0.006141        0.739874       0.005441\n5          0.671414        0.004638        0.688221       0.005762\n6          0.633774        0.003228        0.650904       0.008133\n7          0.606048        0.008341        0.624120       0.008889\n8          0.586223        0.008837        0.605858       0.006527\n9          0.568964        0.005324        0.589065       0.011413\n10         0.555295        0.002455        0.576495       0.008162\n11         0.546593        0.002349        0.570481       0.009861\n12         0.540629        0.001898        0.566814       0.011006\n13         0.528651        0.003696        0.555914       0.008361\n14         0.523607        0.002623        0.552355       0.010605\n15         0.519032        0.001862        0.548983       0.010796\n16         0.514179        0.000913        0.545007       0.011478\n17         0.508824        0.001588        0.541242       0.012881\n18         0.504964        0.002693        0.538327       0.011753\n19         0.499853        0.003439        0.534938       0.010439\n20         0.496571        0.003976        0.533044       0.009761\n21         0.493573        0.003483        0.531449       0.010813\n22         0.491831        0.003357        0.530603       0.010938\n23         0.488524        0.002799        0.528553       0.011084\n24         0.485174        0.002016        0.526113       0.011790\n25         0.481729        0.002600        0.523945       0.011225\n26         0.480254        0.002262        0.523230       0.011452\n27         0.477783        0.001902        0.521776       0.011676\n28         0.474902        0.003005        0.519439       0.010713\n29         0.472486        0.002938        0.518332       0.009949\n30         0.470619        0.003347        0.517276       0.009942\n31         0.468716        0.003182        0.515970       0.009670\n32         0.466677        0.003220        0.515286       0.009412\n33         0.465271        0.003460        0.514172       0.009016\n34         0.463104        0.003572        0.512874       0.008700\n35         0.461313        0.004143        0.512332       0.008410\n36         0.459000        0.003781        0.510704       0.008571\n37         0.457332        0.003890        0.509791       0.008110\n38         0.455053        0.003804        0.508612       0.008641\n39         0.453656        0.003941        0.507963       0.008456\n40         0.451406        0.003268        0.507021       0.008810\n41         0.449972        0.003077        0.506284       0.009194\n42         0.448604        0.002691        0.505271       0.009450\n43         0.447176        0.002342        0.504704       0.009811\n44         0.446304        0.002042        0.504476       0.009743\n45         0.444298        0.002090        0.502807       0.009778\n46         0.442912        0.002268        0.502411       0.009483\n47         0.441760        0.002180        0.502340       0.010093\n48         0.440237        0.002350        0.501567       0.009791\n49         0.437942        0.001547        0.500168       0.010706\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Tuning ETA (Learning Rate)\n- Create a list called eta_vals to store the following \"eta\" values: 0.001, 0.01, and 0.1.\n- Iterate over your eta_vals list using a for loop.\n- In each iteration of the for loop, set the \"eta\" key of params to be equal to curr_val. Then, perform 3-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a DataFrame.\n- Append the final round RMSE to the best_rmse list."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7e0c0602-d655-42aa-800f-66e52f8e13d1"}}},{"cell_type":"code","source":["# Create your housing DMatrix: housing_dmatrix\n# housing_dmatrix = xgb.DMatrix(data=X, label=y) ~ done above\n\n# Create the parameter dictionary for each tree (boosting round)\nparams = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n\n# Create a list of eta values and empty list to store final round rmse per xgboost model\neta_vals = [0.001, 0.01, 0.1]\nbest_rmse = []\n\n# Systimatically vary the eta\nfor curr_val in eta_vals:\n  \n  params[\"eta\"] = curr_val\n  \n  # Perform cross-validation: cv_results\n  cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3,\n                     num_boost_round=10, early_stopping_rounds=5,\n                     metrics=\"rmse\", as_pandas=True, seed=123)\n  \n  # Append the final round rmse to best_rmse\n  best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n  \n# Print the resultant DataFrame\nprint(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\", \"best_rmse\"]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1954748-0065-412d-8f11-f697e370b139"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">     eta  best_rmse\n0  0.001   1.931078\n1  0.010   1.793167\n2  0.100   0.974117\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">     eta  best_rmse\n0  0.001   1.931078\n1  0.010   1.793167\n2  0.100   0.974117\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Tuning max_depth\n- Create a list called max_depths to store the following \"max_depth\" values: 2, 5, 10, and 20.\n- Iterate over your max_depths list using a for loop.\n- Systematically vary \"max_depth\" in each iteration of the for loop and perform 2-fold cross-validation with early stopping (5 rounds), 10 boosting rounds, a metric of \"rmse\", and a seed of 123. Ensure the output is a DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8c705b0-b440-458e-af50-a5ad696346b4"}}},{"cell_type":"code","source":["# Create the parameter dictionary\nparams = {\"objective\":\"reg:squarederror\"}\n\n# Create list of max_depth values\nmax_depths = [2, 5, 10, 20]\nbest_rmse = []\n\n# Systematically vary the max_depth\nfor curr_val in max_depths:\n\n    params[\"max_depth\"] = curr_val\n    \n    # Perform cross-validation\n    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n                 num_boost_round=10, early_stopping_rounds=5,\n                 metrics=\"rmse\", as_pandas=True, seed=123)\n    \n    # Append the final round rmse to best_rmse\n    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nprint(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e9299b6-8a4a-439c-ad45-5067b7831a45"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">   max_depth  best_rmse\n0          2   0.696380\n1          5   0.562416\n2         10   0.541528\n3         20   0.565039\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">   max_depth  best_rmse\n0          2   0.696380\n1          5   0.562416\n2         10   0.541528\n3         20   0.565039\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Tuning colsample_by tree\n\nTune \"colsample_bytree\". You've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) **simply specifies the fraction of features to choose from at every split in a given tree.** In xgboost, colsample_bytree must be specified as a float between 0 and 1.\n\n------------------------------------------------------------------------------------------------------------------------------------------\n- Create a list called colsample_bytree_vals to store the values 0.1, 0.5, 0.8, and 1.\n- Systematically vary \"colsample_bytree\" and perform cross-validation, exactly as you did with max_depth and eta previously."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"46a8d7ca-9e01-4c6e-9aeb-70a3816e4012"}}},{"cell_type":"code","source":["# Create your housing DMatrix\nhousing_dmatrix = xgb.DMatrix(data=X,label=y)\n\n# Create the parameter dictionary\nparams={\"objective\":\"reg:squarederror\",\"max_depth\":3}\n\n# Create list of hyperparameter values: colsample_bytree_vals\ncolsample_bytree_vals = [0.1, 0.5, 0.8, 1]\nbest_rmse = []\n\n# Systamatically vary the hyperparameter value\n\nfor curr_val in colsample_bytree_vals:\n  \n  params['colsample_bytree'] = curr_val\n  \n  # Perform cross-validation\n  cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n               num_boost_round=10, early_stopping_rounds=5,\n               metrics=\"rmse\", as_pandas=True, seed=123)\n    \n    # Append the final round rmse to best_rmse\n  best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n\n# Print the resultant DataFrame\nprint(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b783b4ea-fec5-4886-8b94-c5f527daa9dc"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">   colsample_bytree  best_rmse\n0               0.1   0.829673\n1               0.5   0.670241\n2               0.8   0.644824\n3               1.0   0.623250\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">   colsample_bytree  best_rmse\n0               0.1   0.829673\n1               0.5   0.670241\n2               0.8   0.644824\n3               1.0   0.623250\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Grid search with XGBoost\n\nNow that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously\n\n---------------------------------------------------------------------------------------------------------------------------\n\n- Create a parameter grid called gbm_param_grid that contains a list of \"colsample_bytree\" values (0.3, 0.7), a list with a single value for \"n_estimators\" (50), and a list of 2 \"max_depth\" (2, 5) values.\n- Instantiate an XGBRegressor object called gbm.\n- Create a GridSearchCV object called grid_mse, passing in: the parameter grid to param_grid, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, and 4 to cv. Also specify verbose=1 so you can better understand the output.\n- Fit the GridSearchCV object to X and y.\n- Print the best parameter values and lowest RMSE, using the .best_params_ and .best_score_ attributes, respectively, of grid_mse.\n\n**Note: we are including the GridSearchCV library from sklearn**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5b499ab1-1a5c-4ebb-b2d9-3277ab582c9b"}}},{"cell_type":"code","source":["# Import GridSeachCV\nfrom sklearn.model_selection import GridSearchCV\n\n# Create the parameter grid: gbm_param_grid\ngbm_param_grid = {\n  'colsample_bytree':[0.3, 0.7],\n  'n_estimators':[50],\n  'max_depth':[2, 5]\n}\n\n# Instantiate the regressor: gbm\ngbm = xgb.XGBRegressor()\n\n# Perform grid search: grid_mse\ngrid_mse = GridSearchCV(estimator=gbm, param_grid=gbm_param_grid, \n                        scoring=\"neg_mean_squared_error\", cv=4, verbose=1)\n\n# Fit grid_mse to the data\ngrid_mse.fit(X, y)\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", grid_mse.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b985c066-21f8-45f8-9c77-3e51102a5d13"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Fitting 4 folds for each of 4 candidates, totalling 16 fits\nBest parameters found:  {&#39;colsample_bytree&#39;: 0.7, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 50}\nLowest RMSE found:  0.7223780911219555\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fitting 4 folds for each of 4 candidates, totalling 16 fits\nBest parameters found:  {&#39;colsample_bytree&#39;: 0.7, &#39;max_depth&#39;: 5, &#39;n_estimators&#39;: 50}\nLowest RMSE found:  0.7223780911219555\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Random Search with XGBoost\n\nOften, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter.\n\n---------------------------------------------------------------------------------------------------------------------------------------------------\n\n- Create a parameter grid called gbm_param_grid that contains a list with a single value for 'n_estimators' (25), and a list of 'max_depth' values between 2 and 11 for 'max_depth' - use range(2, 12) for this.\n- Create a RandomizedSearchCV object called randomized_mse, passing in: the parameter grid to param_distributions, the XGBRegressor to estimator, \"neg_mean_squared_error\" to scoring, 5 to n_iter, and 4 to cv. Also specify verbose=1 so you can better understand the output.\n- Fit the RandomizedSearchCV object to X and y.\n\n**Note: we are including the RandomizedSearchCV library from sklearn**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebf4e480-f6ef-480e-95a0-5248f7faa22f"}}},{"cell_type":"code","source":["# Import the RandomizedSearchCV library\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Create the parameter grid: gbm_param_grid\ngbm_param_grid = {\n  'n_estimators':[25],\n  'max_depth':np.arange(2,12)\n}\n\n# Instantiate the regressor: gbm\ngbm = xgb.XGBRegressor(n_estimators=10)\n\n# Perform random search: grid_mse\nrandomized_mse = RandomizedSearchCV(estimator=gbm, param_distributions=gbm_param_grid,\n                                 scoring='neg_mean_squared_error', n_iter=5, cv=4, verbose=1)\n\n# Fit the randomized_mse to the data\nrandomized_mse.fit(X,y)\n\n# Print the best parameters and lowest RMSE\nprint(\"Best parameters found: \", randomized_mse.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"50f965af-40bd-4ea3-ae35-6b6667b6803c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Fitting 4 folds for each of 5 candidates, totalling 20 fits\nBest parameters found:  {&#39;n_estimators&#39;: 25, &#39;max_depth&#39;: 7}\nLowest RMSE found:  0.6747220247596593\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Fitting 4 folds for each of 5 candidates, totalling 20 fits\nBest parameters found:  {&#39;n_estimators&#39;: 25, &#39;max_depth&#39;: 7}\nLowest RMSE found:  0.6747220247596593\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["**The search space size can be massive for Grid Search in certain cases, whereas for Random Search the number of hyperparameters has a significant effect on how long it takes to run.**"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1b1bbf07-4b22-4348-aacb-9ce673ec7e02"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"449bd6f2-cd07-4c76-8ee7-c79c5b270ef3"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Fine_tuning_XGB_Model","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4499686650983585}},"nbformat":4,"nbformat_minor":0}
